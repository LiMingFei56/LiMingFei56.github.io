---
layout: page
permalink: /program-language/java/memory-model
---

Memory Model

## 参考资料 ##
*   > 深入理解java内存模型系列文章，作者程晓明，Java软件工程师，专注于并发编程，就职
      于富士通南大。个人邮箱：asst2003@163.com。
    > http://ifeve.com/java-memory-model-0/

## 什么是线程
> 线程有时被称为轻量级进程（Lightwight Process,LWP），是程序执行流的最小单元。
> 线程是进程中的一个实体，是被系统独立调度和分派的基本单位。线程自己不拥有系统
> 资源，只拥有一点运行时必不可少的资源，它们共享进程所拥有的全部资源。

## 并发编程模型的分类
### 并发编程关注的两个关键问题
*   线程之间的通信：以何种机制实现线程之间交换信息。
*   线程之间的同步：怎么控制不同线程顺序执行的机制。

#### 线程间的通信
    1.共享内存：线程之间共享程序的公共状态，线程通过对公共状态的读和写操作来隐式的进行
    通信。
    2.消息传递：线程之间没有共享公共状态，线程之间需要明确的发送消息来显式的进行通信。

#### 线程间的同步
    1.共享内存方式：显式进行的，明确指定某个方法或某个代码块需要在线程之间互斥执行。
    2.消息传递方式：隐式进行的，因为消息传递是一个先发送，后面一个才能收到。

    注：Java的并发采用的是共享内存模型，Java线程之间的通信总是隐式进行，整个通信过程对
    程序员是完全透明的。

## Java内存模型的抽象
### 内存的分配及共享
> 1.   实例域、静态域和数组元素存储在堆内存，堆内存在线程间共享。
> 2.   局部变量（Local variables）、方法定义参数（Formal method parameters）、异常
       处理器参数（exception handler parameters）放在栈中，不会在线程之间共享。
> > 注：栈是根据数据类型大小自动分配的存储空间，堆是用new，malloc等手动分配的空间。

### Java内存模型（JMM，Java Memory Model）
> JMM决定一个线程对共享变量的写入何时对另一个线程可见，JMM抽象了一个私有本地存储的空间
  ，本地内存存储了该线程以读/写变量的副本。本地内存是抽象概念，不是真实存在，它涵盖了
  缓存，写缓冲区，寄存器以前其他的硬件和编译器的优化。JMM通过控制主内存和每个线程的
  本地内存之间的交互，来实现内存可见性的保护。

## 重排序
### 什么是重排序？
> 在程序执行时为了提高性能，编译器和处理器常常会对指令做重排序。

### 三种重排序
*  编译器优化的重排序：不改变单线程程序语义的情况下，可以重新安排语句的执行顺序。
*  处理器优化的重排序：采用指令集并行技术（Instruction-Level Parallelism，ILP）
   ，对不存在数据依赖性的指令，可以改变执行顺序。
*  内存优化的重排序：使用多个缓冲区，尽量避免Cache Wait。

### 重排序对并发造成的问题
> 有的在后面的指令被安排先执行，就有可能导致多线程程序出现内存可见性的问题。为了保证
  内存可见的一致性，JMM会禁止特定类型的重排序。对于编译器，JMM会禁止特定类型的编译器
  重排序。对于处理器重排序，JMM会要求编译器在生成指令时插入内存屏障指令，来禁止特定
  类型的处理器重排序。

### 重排序必须遵守的语义
> as-if-serial：不管怎么重排序（编译器和处理器为了提高性能），单线程程序运行的结果不
  变。编译器、Runtime、处理器必须遵守as-if-serial语义。所以单线程不存在数据可见性问题
  ，但是多线程有可能存在。因为在多线程之间，不存在数据依赖，所有编译器和处理器会使用重
  排序和猜测（当2个操作存在控制依赖时if(flat) int i = a* a，会先猜测值temp=a*a然后再
  赋值i=temp），会把2个线程执行的操作重排序，可能得到的是一个错误的结果。

## 处理器重排序与内存屏障指令
### 处理器重排序及问题
> 因为现代多处理器都有私有的写缓冲区，处理器对缓冲区的写比对内存的写快很多，所以会先
  写到写缓冲区然后批量写到内存。当一个处理器处理完数据A写到定缓冲器但没有更新到内存
  ，另一个处理器去读A，这时候是读取错误的值，发生数据可见性的问题。
  处理器允许的重排序类型：
  *   Load-Load  ：ia64，PowerPC(ARM)
  *   Load-Store ：ia64，PowerPC
  *   Store-Store：ia64，PowerPC
  *   Store-Load ：sparc-TSO，x86(x64,Amd64),ia64，PowerPC
  sparc-TSO，x86拥有相对较强的处理器内存模型，它们仅允许写-读操作做重排序。

### 内在屏障指令
> 为了保证内存可见性,java编译器在生成指令序列的适当位置会插入内存屏障指令来禁止特定
  类型的处理器重排序。
  JMM把内存屏障指令分为下列四类:
  *   LoadLoad Barriers
  *   StoreStore Barriers
  *   LoadStore Barriers
  *   StoreLoad Barriers
  StoreLoad Barriers是一个“全能型”的屏障，它同时具有其他三个屏障
  的效果。现代的多处理器大都支持该屏障（其他类型的屏障不一定被所有处理器支持）。执行该
  屏障开销会很昂贵，因为当前处理器通常要把写缓冲区中的数据全部刷新到内存中
  （buffer fully flush）。

## happens-before
### 什么是happens-before？
> 从JDK5开始，java使用新的JSR-133内存模型。JSR-133使用happens-before的概念来阐述
  操作之间的内存可见性。在JMM中，如果一个操作执行的结果需要对另一个操作可见，那么这
  两个操作之间必须要存在happens-before关系。这里提到的两个操作既可以是在一个线程之内
  ，也可以是在不同线程之间。

### 与程序员密切相关的happens-before规则
  *   程序顺序规则：一个线程中的每个操作，happens- before 于该线程中的任意后续操作。
  *   监视器锁规则：对一个监视器锁的解锁，happens- before 于随后对这个监视器锁的加锁。
  *   volatile变量规则：对一个volatile域的写，happens- before 于任意后续对这个
      volatile域的读。
  *   传递性：如果A happens- before B，且B happens- before C，那么A
      happens- before C。

### happens-before只定义操作的可见，不要求执行的顺序。
> happens-before不是要求前一个操作一定在后一下操作前执行，只是定义前一个操作的结果对后
  一个操作可见。一个happens-before规则通常对应多个编译器和处理器重排序规则。

    int a = 1;
    int b = 2;
    int c = a * b;

> a happens-before c + b happens-before c => a happens-before b
  但是a和b没有数据依赖，所以不必保证a操作在b操作前面完成。

## 数据竞争与顺序一致性保证
### 数据竞争
> 在一个线程写数据，另一个线程读数据，而且写和读没有通过同步来排序。这样就存在数据竞争，
  当代码中包含数据竞争里，程序的执行往往产生违反直觉的结果。

### 理想化的顺序一致性保证
> 顺序一致性内存模型是一个被计算机科学家理想化了的理论参考模型
  ，它为程序员提供了极强的内存可见性保证。

> JMM和顺序一致性模型的区别：

  1.    顺序一致性模型保证单线程内的操作会按程序的顺序执行，而JMM不保证单线程内的操作
  会按程序的顺序执行（比如上面正确同步的多线程程序在临界区内的重排序）。
  2.    顺序一致性模型保证所有线程只能看到一致的操作执行顺序，而JMM不保证所有线程能看
  到一致的操作执行顺序。
  3.    JMM不保证对64位的long型和double型变量的读/写操作具有原子性，而顺序一致性模型
  保证对所有的内存读/写操作都具有原子性。

## Volatile
### Volatile 特性
1.   可见性。对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后
     的写入。
2.   原子性：对任意单个volatile变量的读/写具有原子性，但类似于volatile++这种复合操作
     不具有原子性。

### Volatile 内存语义
1.   线程A写一个volatile变量，实质上是线程A向接下来将要读这个volatile变量的某个线程
     发出了（其对共享变量所在修改的）消息。
2.   线程B读一个volatile变量，实质上是线程B接收了之前某个线程发出的（在写这个volatile
     变量之前对共享变量所做修改的）消息。
3.   线程A写一个volatile变量，随后线程B读这个volatile变量，这个过程实质上是线程A通过
     主内存向线程B发送消息。

### Volatile 内存语义的实现
1.   当第二个操作是volatile写时，不管第一个操作是什么，都不能重排序。这个规则确保
     volatile写之前的操作不会被编译器重排序到volatile写之后。
2.   当第一个操作是volatile读时，不管第二个操作是什么，都不能重排序。这个规则确保
     volatile读之后的操作不会被编译器重排序到volatile读之前。
3.   当第一个操作是volatile写，第二个操作是volatile读时，不能重排序。

### Volatile 内存语义的保守实现
1.   在每个volatile写操作的前面插入一个StoreStore屏障。
2.   在每个volatile写操作的后面插入一个StoreLoad屏障。
3.   在每个volatile读操作的后面插入一个LoadLoad屏障。
4.   在每个volatile读操作的后面插入一个LoadStore屏障。
> JMM在实现上的一个特点：首先确保正确性，然后再去追求执行效率。上述内存屏障插入策略
  非常保守，但它可以保证在任意处理器平台，任意的程序中都能得到正确的volatile内存语义。
  上面的策略可以根据不同平台来进行优化，比如x86平台是关注StoreLoad。

### Volatile与锁
> 由于volatile仅仅保证对单个volatile变量的读/写具有原子性，而锁的互斥执行的特性可以
  确保对整个临界区代码的执行具有原子性。在功能上，锁比volatile更强大；在可伸缩性和
  执行性能上，volatile更有优势。

## 锁
### 内存锁的语义
1.   线程A释放一个锁，实质上是线程A向接下来将要获取这个锁的某个线程发出了（线程A对
     共享变量所做修改的）消息。
2.   线程B获取一个锁，实质上是线程B接收了之前某个线程发出的（在释放这个锁之前对共享
     变量所做修改的）消息。
3.   线程A释放锁，随后线程B获取这个锁，这个过程实质上是线程A通过主内存向线程B发送消息。

### 内存锁的语义实现
#### 公平锁
> 公平锁在释放锁的最后写volatile变量state；在获取锁时首先读这个volatile变量。
  根据volatile的happens-before规则，释放锁的线程在写volatile变量之前可见的共享变量
  ，在获取锁的线程读取同一个volatile变量后将立即变的对获取锁的线程可见。

#### 非公平锁
> compareAndSetState方法同时具有Volatile写和读的内存语义。

#### x86实现
> 在多处理上对需要原子操作的指令前加上lock指令，该指令会锁住总线，不准其他的处理器访问，
  这样的效率很底，在有些情况下可以使用Cache Locking。

#### concurrent包的实现
1. 首先，声明共享变量为volatile；
2. 然后，使用CAS的原子条件更新来实现线程之间的同步；
3. 同时，配合以volatile的读/写和CAS所具有的volatile读和写的内存语义来实现线程之间
   的通信。

## final
### final编译器和处理器重排序规则
1. 在构造函数内对一个final域的写入，与随后把这个被构造对象的引用赋值给一个引用变量
   ，这两个操作之间不能重排序。
2. 初次读一个包含final域的对象的引用，与随后初次读这个final域，这两个操作之间不能
   重排序。

### final基本类型写重排序
1. JMM禁止编译器把final域的写重排序到构造函数之外。
2. 编译器会在final域的写之后，构造函数return之前，插入一个StoreStore屏障。这个屏障
   禁止处理器把final域的写重排序到构造函数之外。

### final基本类型读重排序
1. 在一个线程中，初次读对象引用与初次读该对象包含的final域，JMM禁止处理器重排序这
   两个操作（注意，这个规则仅仅针对处理器）。编译器会在读final域操作的前面插入一个
   LoadLoad屏障。

### final引用类型重排序
1. 在构造函数内对一个final引用的对象的成员域的写入，与随后在构造函数外把这个被构造
   对象的引用赋值给一个引用变量，这两个操作之间不能重排序。

### 为什么final引用不能从构造函数内“逸出”
> 构造函数内部可以允许重排序，当引用逸出并且是在final初始化前面，那么读的操作有可能是
  读取final对象初始前的值。

